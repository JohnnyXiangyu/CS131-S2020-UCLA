\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}

% inlined bib file
\usepackage{filecontents}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Exploration of Python Concurrency with Asyncio}

%for single author (just remove % characters)
\author{
{\rm Xiangyu Wan}\\
UCLA
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle


%-------------------------------------------------------------------------------
\section{Overall}
%-------------------------------------------------------------------------------

Through this project, I am convinced that asyncio is overall a good framework for building a server herd like one that I created in this project.
Creating and maintaining applications with Asyncio is fairly easy, and the flexibility provided by Python's dynamic type system enables great magnitude of code reusing, which further simplifies the implementation.
Compared to Java's JMM real multithreading system, the single-thread, event-based Asyncio library eliminates potential problem of data races, yet poses a strict limit on maximum workload.
Node.js, in addition to java, is yet another interesting competitor of asyncio that's worth discussing.

%-------------------------------------------------------------------------------
\section{Simplicity}
%-------------------------------------------------------------------------------

To begin with, implementing a concurrent server using asyncio is fairly easy. 
The framework provides a simple, concise yet powerful interface for creating concurrent execution units, the await/async system. 
To utilize this interface, one simply needs to declare a function with keyword async, which effectively turns it into a coroutine. 

The syntax to call the function then becomes "await xxx()", or, introduced in Python3.8, "asyncio.run(xxx())". 
Using this syntax in the program will effectively turn the program's concurrent part into a composition of coroutines. 
The programmer is reliefed from this point on: under the cover asyncio system does all the dirty jobs. 
Asyncio schedules all coroutines supplied for execution using a deterministic sequence
In the scope of every single task, it's run sequentially until an await appears. 
The system will then yield that caller routine, go into the async routine, execute until another await is encountered or the function returns. 
When a coroutine, probably under several recursions into each await call, actually awaits on a IO operation that was naturally blocking without concurrent design, the system yields this coroutine and immediately schedules the next coroutine waiting to be scheduled.  
This layer of abstraction enables high level of flexibility in terms of program structure, namely it can reconstructed as freely as how a non-concurrent program can be reconstructed. 
With proper usage of async and await/async.run(), a programmer can easily break down any code that needs reconstruction and put the separated logic in different coroutines, or maybe in normal functions if they are not IO bound.
For example, in my inplementation, the server main routine is designed like this:
\begin{lstlisting}[breaklines=true]
async def serve_client(self, reader, writer):
    msg_raw = await reader.read(self.message_max_length)
    msg = msg_raw.decode()
    incoming_data = self.filterMessage(msg)

    if incoming_data == False:
        err_response = f'? {msg}'
        writer.write(err_response.encode())
        await writer.drain()

    elif incoming_data['type'] == 'IAMAT':
        ... # process IAMAT request

    elif incoming_data['type'] == 'AT':
        ... # process AT request

    elif incoming_data['type'] == 'WHATSAT':
        ... # process WHATSAT request
    
    # finalize
    writer.close()
    self.m_logger.printFile('')
    if dev_flags.debug:
        print(self.database)
\end{lstlisting}
Overall the function is defined as a coroutine, and inside it consists of several calls to both coroutines, including reading input from a reader stream, and normal functions, parsing input message, for example.
It's easy to break this massive and need-to-parallelize code into parts that may be parallelizable or not,
and the process is not too different from designing the same working logic as a single-loop service.
In fact, simply removing asyncio keywords results in exactly a working non-concurrent implementation.
Such level of flexibility will also contribute to an easy process of maintainance. 
Adding new or modifying existing modules can happen just like adding a function, for either asynchronous or sequential routines, into the existing structure.

Besides code structure, I/O api provided by asynio is also simple and concise.
Take a look at the following code snipet:
\begin{lstlisting}[breaklines=true]
async def broadcast(self, message="", exclusion=[]):
    for neighbour in self.neighbours:
        try:
            ... # exclude some servers
            n_ip = '127.0.0.1'
            n_port = isc.port_numbers[neighbour]
            reader, writer = await asyncio.open_connection(n_ip, n_port)
            writer.write(message.encode())
            await writer.drain()  # is this line necessary here?
            writer.close()
        except ConnectionRefusedError: 
            ... # handle server-down scenario
\end{lstlisting}
This small piece of code is how I implemented the flooding algorithm propagating message to neighbouring servers.
To contact another server, one would only need to specify IP, port, and the message byte stream, and the system does all the rest.
The interface allows inter-server communication to be implemented easily, further facilitating creation of a server herd.

%-------------------------------------------------------------------------------
\section{Dynamic Typing System}
%-------------------------------------------------------------------------------

Besides features exclusive to asyncio, Python also provides a crucial feature that can be used in buidling server herds: its dynamic type system.
Compared to the static typed C family, or Java's inheritance multimorphism, Python provides a far more flexible implementation of duck typing idology.
Namely, when writing Python applications, one can put whatever object, as everything in Python is an object, into a statement's variable, and as long as the object methods are implemented by and thus callable from the assigned object, the program works.

One implication of dynamic typing is simple yet powerful implementation of exception handling.
Instead of throwing exceptions or defining special wrapping classes to enable special values to alarm an exception,
a Python function can simply return a value of different type, from the value of interest, to notify its caller there's exception.
For example, in my server.py the message parser is implementated as such:
\begin{lstlisting}[breaklines=true]
def filterMessage(self, message=""):
    incoming_type = ""
    ... # if it's grammarly valid
    else:
        self.m_logger.printFile(f'Invalid input:\n  content: {message}')
        return False
    self.m_logger.printFile(
        f'Incoming transmission passes grammar check.\n  type: {incoming_type}\n  content: {message}')

    data = self.parseMessage(message, incoming_type)
    if data['type'] == 'ERROR':
        self.m_logger.printFile(f'  Invalid argument range: {message}')
        return False
    else:
        return data
\end{lstlisting}
Python's dynamic typing system allows me to simply return False instead of a dictionary when the server receives an invalid message.
Upon return, the service routine checks if this return value is the F boolean before anything can happen.
This chekcing procedure is both very simple and scalable. 
Although my example is not a good one in the sense that it shows the scalability, we can bring the discussion beyond the scope of a server herd supporting merely 3 requests.
If we maintain the same structure for both normal and exceptional return values, it's practically limiting the range we can choose from for valid returns.
Words that represent error can always collide with the phrasing of some special message syntax from the client, e.g. when client actually need to send error messages.
It's more flexible to make exceptional return differnt type.

Moreover, although this has been mentioned above over and over, Python's dynamic typing system generates simpler and more maintainable code.
In C or Java, there have to be extra piece, or even chunks, of code to define a way to distinguish between normal and exceptional values.
Minor changes to the data structure might trigger a series of changes, since exception hassle with normal values.
The other way around, throwing exceptions, can bring no simpler syntax either.

%-------------------------------------------------------------------------------
\section{Event-based vs. Mutithreaded}
%-------------------------------------------------------------------------------

Asyncio itself, with its Python3 environment, as discussed before, shows good suitability for the job of building a server herd.
Beyond just Python, however, there are other object oriented languages that also has implementation for concurrency.
Java, for example, is one of those famous for its implementation of concurrency with a complicated Java Memory Model, which grants application the power to really spawn and execute multiple threads together.

Asyncio is, however, entirely different from this counterpart.
Java's concurrency is implemented by multithreading, that the CPU actually runs multiple part of the program at the same time;
in asyncio, however, the same, or at least similar, effect is implemented by a event loop, essentially a single-thread workaround for concurrency.

As studied in Assignment 3, Java has a entire system of enormous complexity to handle concurrency: Java Memory Model. 
The major job of this module is to handle memory management across threads, as data races are the biggest hinder between ultimate concurrency and reality. 
A list of models are implemented for programmers to choose, each serving its purpose with different advantages and disadvantages.

Asyncio, on the other hand, doesn't need to care about data race at all.
The low level implementation of concurrency is not multiple cores running at the same time, but a single thread in which tasks rapidly circulate around.
This event loop system implements its own scheduler, and runs tasks in the order of their scheduling.
Tasks in this event loop circulate fast enough to create this illusion of concurrent execution.
As mentioned above, when some operatons requiring waiting for IO is executed, instead of waiting for that to be done as normal execution order, the scheduler deschedules the caller task, forcing it to yield, then schedules the next task in the queue.
Since there's technically only one operation at any time point, data race can be just avoided totally.

The absense of data races benefits both the simplicity and efficiency of a concurrent program.
Without the burden of taking care of synchronization across all the threads created, the programmer can write much simpler code for each task, and then supply them to the event loop knowing memory can't be messed. 
For example, in both C and Java, programmer frequently need to declare and use locks.
Using asyncio means the concurrent program is as safe as its sequential cousin without any explicit effort from the programmer.

The efficiency boost also revolves heavily on locking mechanisms. 
As demonstraded in Assignment 3, using locks, besides forcing the program to run less concurrently, brings about a significant amount of performance overhead.
Much of CPU time are spent on creating, setting, and freeing locks as well as waking and sleeping threads. 
Especially when critical sections are short and frequent, such overhead can even take over the entire program's performance measurement.
In such cases, it's probably better to execute an event loop instead of the heavy way of using threads. 
Since there's only one thread, no lock operations are necessary to keep different execution units synchronized. 
It also don't involve system calls that control the state of threads.
System calls are much more expensive than software simulations of a scheduler.
In the scope of a single thread, event-based asyncio framework results in much better usage efficiency of its CPU power, as time wasting happens at its minimal potential.

All these advantages of asyncio concurrency, however, don't prevail in large applications as well as in smaller ones.
In the scope of the simple server herd created for this project, asyncio is more than enough, since the normal and maximum workload exerted onto the server is within a tight limit.
Scaling this service up, however, event-based concurrency may start to show its weakness.
For example, say the final deployment of our product requires the ability to serve 10000 users at a single time, instead of how they serve at most a dozen in project testing, single threaded application will hit its bottleneck, that no matter how well we can utilize the one thread we have, it might not be enough to process such a big workload at an instant.
In this case, it becomes an disadvantage for asyncio to implement concurrency with event-loop.

Theoretically, however, there is a way to get around that bottleneck of a single thread.
As mentioned above, inter-server communication can be easily implemented using asyncio.
With a proper design of protocols, it might be possible to implement a system that distributes incoming work to different servers, and have a concurrent processing of information this way.
However this argument can hardly be called a solution, as I'm not yet capable of trying it out and stress test its validity.

In a brief conclusion, the event-based strategy employed by asyncio, compared to Java's multithreading model, prevails with simplicity and light weight execution.
Applications can be created to achieve similar level of concurrency without worrying about locking syntax or performance overhead of managing multiple threads.
However, when the project scales up to a extent where the server is required to handle heavier workloads, these advantages turn against asyncio. 
The event system won't be able to circulate fast enough when the workload goes beyond its capacity.

%-------------------------------------------------------------------------------
\section{Compared to Node.js Event-based Concurrency}
%-------------------------------------------------------------------------------

While Java and asyncio differ entirely on both implementation and strength/weakness, another contender for asyncio, the more popular server framework Node.js, plays the same role in a very close way as asyncio.
They are both event-based, and both have fairly simple syntax for waiting for I/O operations.

To begin with, Node.js has an asynchronous syntax almost identical to the await/async interface from asyncio.
For example, look at the following code snipet:
\begin{lstlisting}[breaklines=true]
// example of javascript async/await
function resolveAfter2Seconds(x) { 
    return new Promise(resolve => {
        setTimeout(() => {
        resolve(x);
        }, 2000);
    });
}
    
async function f1() {
    var x = await resolveAfter2Seconds(10);
    console.log(x); // 10
}
    
f1();
\end{lstlisting}
Asyncio example can be found in previous sections.
It's easy to spot the resemblance between asyncio and javascript async syntax.

There is but a subtle different, that in asycio, await keyword needs to be applied to all functions/methdos defiend as async, as they are essentially defined to be a special object instead of functions;
in the code above, however, the top-level function f1() is called directly.
This is reflecting the different strucutre of event loop in asyncio and Node.js.
For asyncio application, all coroutines are not function but coroutine objects, and they need to be submitted to the asyncio system to be scheduled and executed.
Inside the event loop, once initiated, asyncio system actively circulates all coroutines and handles awaits within them.

In Node.js, or just javascript since await is a javascript feature, async is not explicitly turninig a function into a coroutine object; 
rather, it's more of declaring the ability of a function to use the powerful await expression.
Using await will pause the execution of this function, and continues to later ones from the same caller, until the object being waited, called promise, is "fulfilled".
When the promise is fulfilled, the async function continues to execute its remaining code, usually handling the value returned from the promise.

Asyncio library employs a similar system of handling unfulfilled values. 
When a coroutine is called with await, a "future" is returned, and the caller is automatically yielded to wait for the future to fulfill.
Once fulfilled, the remaining code in that asynchronous function can be scheduled again and continue to execute, just like how Node.js handles promise fulfilling.
The major difference is that when a function waits, asyncio schedules another ready coroutine and would actually wait if only one coroutine is waited, whereas with javascript it's going to execute the following code from high level caller.

Besides this syntax, however, there's an alternative way of running concurrent program in Node.js.
Instead of declaring functions asynchronous and waiting for promises in them, programmer can also assign callbacks to a promise, in the form of arrow functions, which handles fulfillment and rejection.
For example, this is a fetch() call I wrote in a react project:
\begin{lstlisting}[breaklines=true]
fetch(url, {
    method: "POST",
    cache: "no-cache",
    headers: {
        "Content-type": "application/json",
    },
    redirect: "follow",
    body: JSON.stringify(credential),
})
    .then((res) => res.json())
    .then((res) => {
        if (res.valid === "yes") {
            this.props.onSuccess(res.key, credential);
        }
        else {
            this.setState({state: "loginFail"});
        }
    });
\end{lstlisting}
The return of fetch() function is a promise.
Instead of waiting for the promise, in contrast with previous example, I chained it to a series of callbacks defined in .then() methods.
When this fetch().then().then() statement is executed, since promise is not fulfilled, Node.js system will continue on executing the later expressions, pretending this statement has nothing to do with later ones.
Once the promise is fulfilled, its value is passed into callback function defined in .then() method.
In this particular example, the first then() method returns another promise that's chained to the second then() method, further parallelizing the following execution.

The most significant difference from here is that, using callbacks allows the program to wait inside the same scope.
In the previous approach, one that's identical to asyncio's, the waiting function has to exit itself before its promise/future is fulfilled.
With that change of scope, the program loses record of the calling function's local variables.
There are cases, however, where one doesn't want the whole function to pause for a single asynchronous IO to finish.
To achieve that, he would need to write levels of wrappers to fully separate asynchronous calls which require, say, the same set of local variables as input.
Using the Node.js method, in contrast, one can just put all the asynchronous calls sequentially, with the confidence that they'll be initiated one after another, and other parts of the remaining routine can be executed before some other task begin scheduled.

The callback takes care of processing of fulfilled value.
In my fetch() example here, when response fulfills, it's first asynchronously converted to an object, then passed to a value checking routine.
Upon different results of checking, the React object transitions to a different state, as the final effect of a response from my post request.
The code after this fetch() call, however, gets executed before all these happen.
For example, local information can be immedaitely logged into the console, or a log file if this is the server code.
Retaining of local scope grants Node.js application extra power that it enables a more flexible and responsive style of coding than asyncio.

%-------------------------------------------------------------------------------
\section{Conclusion}
%-------------------------------------------------------------------------------

In conclusion, asyncio is overall a great fit for creating a small server herd, since it's got great simplicity, both syntactical and in terms of implementation.
With Python's dynamic type nature and a well-designed event-loop, it's highly efficient on a single thread.
When the project scales up, however, asyncio may hit its throughput ceiling since the responsiveness of single thread application drops horrifically under a massive amount of workload, one that may be divided into multiple threads.
These characteristics are also the clear distinction between Java concurrency and asyncio.
Lastly, although Node.js can run in the same way as asyncio's await/async interface, this close competitor of asyncio's also provides a, potentially more powerful, alternative syntax of callbacks, thus Node.js enables a wider range of jobs.

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
% \bibliography{\jobname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks